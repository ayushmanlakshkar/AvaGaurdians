{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":818100,"sourceType":"datasetVersion","datasetId":430503},{"sourceId":7423653,"sourceType":"datasetVersion","datasetId":4319381},{"sourceId":7436830,"sourceType":"datasetVersion","datasetId":4328277},{"sourceId":7440780,"sourceType":"datasetVersion","datasetId":4330782},{"sourceId":7440861,"sourceType":"datasetVersion","datasetId":4330828}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport sklearn as skl\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-20T20:56:43.224361Z","iopub.execute_input":"2024-01-20T20:56:43.224838Z","iopub.status.idle":"2024-01-20T20:56:57.662716Z","shell.execute_reply.started":"2024-01-20T20:56:43.224801Z","shell.execute_reply":"2024-01-20T20:56:57.659425Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/avanche-additional-features/Avalanche.csv\n/kaggle/input/avalanche-snow-stability-dataset/snow_instability_field_data.csv\n/kaggle/input/avalanche-snow-stability-dataset/read-me.pdf\n/kaggle/input/avalanche-forecasting-prediction/StevensPass-GraceLakes_4790_feet_2019.csv\n/kaggle/input/avalanche-accurate-features/Avalanche_accurate_featuers.csv\n/kaggle/input/avalanche-forecasting-dataset/data_rf1_forecast.csv\n/kaggle/input/avalanche-forecasting-dataset/dataset3_forecast.csv\n/kaggle/input/avalanche-forecasting-dataset/read_me.xlsx\n/kaggle/input/avalanche-forecasting-dataset/dataset3_nowcast.csv\n/kaggle/input/avalanche-forecasting-dataset/dataset2.csv\n/kaggle/input/avalanche-forecasting-dataset/data_rf2_tidy.csv\n/kaggle/input/avalanche-forecasting-dataset/dataset1.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"st_df = pd.read_csv('/kaggle/input/avalanche-snow-stability-dataset/snow_instability_field_data.csv',delimiter = ';')\nad_df = pd.read_csv('/kaggle/input/avanche-additional-features/Avalanche.csv',delimiter = ',')\nac_df = pd.read_csv('/kaggle/input/avalanche-accurate-features/Avalanche_accurate_featuers.csv',delimiter= ',')\n\n# new_df = pd.DataFrame()\n# new_df['Elevation '] = ad_df['Temperature']\n\n\nst_df['Elevation (m a.s.l.)'].fillna(int(st_df['Elevation (m a.s.l.)'].mean()),inplace=True)\nst_df['Avalanche_activity'].fillna(int(st_df['Avalanche_activity'].mean()),inplace=True)\nst_df['RF_Regional _danger_level_forecast'].fillna(int(st_df['RF_Regional _danger_level_forecast'].mean()),inplace=True)\nprint(st_df.shape)\nprint(ad_df.shape)\nprint(ac_df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:56:57.664686Z","iopub.execute_input":"2024-01-20T20:56:57.665431Z","iopub.status.idle":"2024-01-20T20:56:57.723159Z","shell.execute_reply.started":"2024-01-20T20:56:57.665391Z","shell.execute_reply":"2024-01-20T20:56:57.722076Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(599, 57)\n(50, 19)\n(204, 6)\n","output_type":"stream"}]},{"cell_type":"code","source":"ac_df['Prediction'].head()","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:56:57.724470Z","iopub.execute_input":"2024-01-20T20:56:57.724893Z","iopub.status.idle":"2024-01-20T20:56:57.734852Z","shell.execute_reply.started":"2024-01-20T20:56:57.724865Z","shell.execute_reply":"2024-01-20T20:56:57.733594Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0    2\n1    0\n2    0\n3    1\n4    0\nName: Prediction, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"\n\n# Assuming df1 and df2 are your dataframes\ndf1 = pd.DataFrame({'danger_level': st_df['RF_Regional _danger_level_forecast'].astype(int),\n                    'elevation': st_df['Elevation (m a.s.l.)'] ,'avalanche_act' : st_df['Avalanche_activity']})\nprint(df1['danger_level'].unique())\ndf2 = pd.DataFrame({'danger_level': ac_df['Prediction'].div(max(ac_df['Prediction'])+1).mul(5).round(0).astype(int),\n                    'temperature': ac_df['Air Temperature'],'wind_speed': ac_df['Wind']})\nad_df['tilt_magnitude'] = np.sqrt(ad_df['tilt-x']**2 + ad_df['tilt-y']**2 + ad_df['tilt-z']**2)\n\ndf3 = pd.DataFrame({'danger_level': 6 - ad_df['tilt_magnitude'].div(max(ad_df['tilt_magnitude'])+1).mul(5).round(0).astype(int),'humidity': ad_df['Humidity']})\n\nprint(df2['danger_level'].unique())\nprint(df3['danger_level'].unique())\n# Merge dataframes on the 'danger_level' k\nmerged_df = pd.merge(df1, df2, on='danger_level',how = 'inner') \nmerged_df = pd.merge(merged_df,df3, on='danger_level',how='right')\n\n# Display the merged dataframe\n# df.to_csv(csv_file_path, index=False)\nmerged_df.head(100)\nprint(merged_df['danger_level'].unique())\nprint(merged_df.describe())\nfor i in merged_df.columns:\n    merged_df[i].fillna(int(merged_df[i].mean()),inplace=True)\nprint(merged_df.describe())","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:56:57.736687Z","iopub.execute_input":"2024-01-20T20:56:57.737612Z","iopub.status.idle":"2024-01-20T20:56:58.513696Z","shell.execute_reply.started":"2024-01-20T20:56:57.737581Z","shell.execute_reply":"2024-01-20T20:56:58.512641Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[2 3 1 4]\n[3 0 2]\n[2 3 4 1]\n[2 3 4 1]\n        danger_level      elevation  avalanche_act    temperature  \\\ncount  882249.000000  882244.000000  882244.000000  882244.000000   \nmean        2.335440    2444.215818       0.206535      -8.673952   \nstd         0.472156     186.976027       0.404819      14.688612   \nmin         1.000000    1760.000000       0.000000     -34.000000   \n25%         2.000000    2360.000000       0.000000     -22.000000   \n50%         2.000000    2460.000000       0.000000      -8.000000   \n75%         3.000000    2560.000000       0.000000       3.000000   \nmax         4.000000    2965.000000       1.000000      15.000000   \n\n          wind_speed       humidity  \ncount  882244.000000  882249.000000  \nmean       22.338598      58.677484  \nstd        11.495827       4.882538  \nmin         6.000000      48.000000  \n25%        11.000000      55.000000  \n50%        22.000000      59.000000  \n75%        35.000000      63.000000  \nmax        40.000000      68.000000  \n        danger_level      elevation  avalanche_act    temperature  \\\ncount  882249.000000  882249.000000  882249.000000  882249.000000   \nmean        2.335440    2444.215817       0.206534      -8.673948   \nstd         0.472156     186.975497       0.404818      14.688571   \nmin         1.000000    1760.000000       0.000000     -34.000000   \n25%         2.000000    2360.000000       0.000000     -22.000000   \n50%         2.000000    2460.000000       0.000000      -8.000000   \n75%         3.000000    2560.000000       0.000000       3.000000   \nmax         4.000000    2965.000000       1.000000      15.000000   \n\n          wind_speed       humidity  \ncount  882249.000000  882249.000000  \nmean       22.338596      58.677484  \nstd        11.495794       4.882538  \nmin         6.000000      48.000000  \n25%        11.000000      55.000000  \n50%        22.000000      59.000000  \n75%        35.000000      63.000000  \nmax        40.000000      68.000000  \n","output_type":"stream"}]},{"cell_type":"code","source":"max(ad_df['tilt_magnitude'])","metadata":{"execution":{"iopub.status.busy":"2024-01-20T19:58:15.407425Z","iopub.execute_input":"2024-01-20T19:58:15.408754Z","iopub.status.idle":"2024-01-20T19:58:15.417090Z","shell.execute_reply.started":"2024-01-20T19:58:15.408700Z","shell.execute_reply":"2024-01-20T19:58:15.415880Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"12.29186723000212"},"metadata":{}}]},{"cell_type":"code","source":"merged_df.columns\n\n#snow depth , slope angle, snow thickness , temperature  , relative humidity","metadata":{"execution":{"iopub.status.busy":"2024-01-20T19:58:15.418421Z","iopub.execute_input":"2024-01-20T19:58:15.418753Z","iopub.status.idle":"2024-01-20T19:58:15.428801Z","shell.execute_reply.started":"2024-01-20T19:58:15.418722Z","shell.execute_reply":"2024-01-20T19:58:15.427750Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Index(['danger_level', 'elevation', 'avalanche_act', 'temperature',\n       'wind_speed', 'humidity'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"\n\nX = merged_df[['elevation','temperature','wind_speed','humidity']]\ny = merged_df['danger_level'].div(5).mul(100).astype(int)\n# y = st_df[['RF_Regional _danger_level_forecast']].div(5)\ny.tail()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T19:58:15.430305Z","iopub.execute_input":"2024-01-20T19:58:15.430847Z","iopub.status.idle":"2024-01-20T19:58:15.463029Z","shell.execute_reply.started":"2024-01-20T19:58:15.430808Z","shell.execute_reply":"2024-01-20T19:58:15.461871Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"882244    40\n882245    40\n882246    40\n882247    40\n882248    40\nName: danger_level, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n# X = sc.fit_transform(X)\n\n# from sklearn.preprocessing import OneHotEncoder\n# ohe = OneHotEncoder()\n# y = ohe.fit_transform(y).toarray()\n\nfrom sklearn.model_selection import train_test_split\ntrain_x,X_test,train_y,y_test = train_test_split(X,y,test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-01-20T19:58:15.464866Z","iopub.execute_input":"2024-01-20T19:58:15.465562Z","iopub.status.idle":"2024-01-20T19:58:15.794631Z","shell.execute_reply.started":"2024-01-20T19:58:15.465515Z","shell.execute_reply":"2024-01-20T19:58:15.793375Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_x.shape\ntrain_y.head()\n# X_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:56:38.034664Z","iopub.execute_input":"2024-01-20T20:56:38.035418Z","iopub.status.idle":"2024-01-20T20:56:38.363034Z","shell.execute_reply.started":"2024-01-20T20:56:38.035360Z","shell.execute_reply":"2024-01-20T20:56:38.361230Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:56:38.034664Z","iopub.execute_input":"2024-01-20T20:56:38.035418Z","iopub.status.idle":"2024-01-20T20:56:38.363034Z","shell.execute_reply.started":"2024-01-20T20:56:38.035360Z","shell.execute_reply":"2024-01-20T20:56:38.361230Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_x\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      2\u001b[0m train_y\u001b[38;5;241m.\u001b[39mhead()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# X_test.head()\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"],"ename":"NameError","evalue":"name 'train_x' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=200)\n# train_x.fillna(int(train_x.mean()),inplace=True)\nhistory=model.fit(train_x, np.ravel(train_y,order='C'))\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n# train_x_scaled = scaler.fit_transform(train_x)\n\n# test_x_scaled = scaler.transform(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T19:59:46.755098Z","iopub.execute_input":"2024-01-20T19:59:46.755597Z","iopub.status.idle":"2024-01-20T20:06:41.783905Z","shell.execute_reply.started":"2024-01-20T19:59:46.755561Z","shell.execute_reply":"2024-01-20T20:06:41.782512Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:07:48.401794Z","iopub.execute_input":"2024-01-20T20:07:48.402268Z","iopub.status.idle":"2024-01-20T20:07:50.545923Z","shell.execute_reply.started":"2024-01-20T20:07:48.402232Z","shell.execute_reply":"2024-01-20T20:07:50.544396Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"1.0"},"metadata":{}}]},{"cell_type":"code","source":"\ny_predicted = model.predict(X_test)\n     ","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:10:33.872859Z","iopub.execute_input":"2024-01-20T20:10:33.873488Z","iopub.status.idle":"2024-01-20T20:10:36.013495Z","shell.execute_reply.started":"2024-01-20T20:10:33.873443Z","shell.execute_reply":"2024-01-20T20:10:36.012477Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_predicted)\ncm","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:10:51.759623Z","iopub.execute_input":"2024-01-20T20:10:51.760045Z","iopub.status.idle":"2024-01-20T20:10:51.866861Z","shell.execute_reply.started":"2024-01-20T20:10:51.760013Z","shell.execute_reply":"2024-01-20T20:10:51.865556Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"array([[    1,     0,     0],\n       [    0, 58824,     0],\n       [    0,     0, 29400]])"},"metadata":{}}]},{"cell_type":"code","source":"merged_df.to_csv('mycsvfile.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-20T19:59:28.520994Z","iopub.status.idle":"2024-01-20T19:59:28.521414Z","shell.execute_reply.started":"2024-01-20T19:59:28.521189Z","shell.execute_reply":"2024-01-20T19:59:28.521207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score\nfrom sklearn.datasets import make_circles\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\n\ny_val_pred=model.predict(X_test)\n\nprint(precision_score(y_test,y_val_pred,average=None))\n     \n\n\nprint(recall_score(y_test,y_val_pred,average=None))\n     \n\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_val_pred))","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:12:05.526741Z","iopub.execute_input":"2024-01-20T20:12:05.527211Z","iopub.status.idle":"2024-01-20T20:12:07.920694Z","shell.execute_reply.started":"2024-01-20T20:12:05.527179Z","shell.execute_reply":"2024-01-20T20:12:07.919401Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[1. 1. 1.]\n[1. 1. 1.]\n              precision    recall  f1-score   support\n\n          20       1.00      1.00      1.00         1\n          40       1.00      1.00      1.00     58824\n          60       1.00      1.00      1.00     29400\n\n    accuracy                           1.00     88225\n   macro avg       1.00      1.00      1.00     88225\nweighted avg       1.00      1.00      1.00     88225\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# from keras.models import Sequential\n# from keras.layers import Dense, Dropout\n# from keras.utils import to_categorical\n# from sklearn.model_selection import train_test_split\n# from keras.optimizers import Adam\n\n# # Assuming train_x and train_y are your training data and labels\n# # Split the data into training and testing sets\n# train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# # Assuming train_y is a Pandas Series or NumPy array\n# # Convert the target variable to categorical\n# train_y_categorical = to_categorical(train_y)\n# test_y_categorical = to_categorical(test_y)\n\n# # create model\n# model = Sequential()\n\n# # add model layers\n# model.add(Dense(50, activation='relu', input_dim=2))\n# model.add(Dropout(0.2))\n# model.add(Dense(25, activation='relu'))\n# model.add(Dropout(0.1))\n# model.add(Dense(16, activation='relu'))\n# model.add(Dropout(0.1))\n# model.add(Dense(5, activation='softmax')) \n\n# custom_optimizer = Adam(learning_rate=0.002)\n\n# # Compile the model with the custom optimizer\n# model.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['accuracy'])\n# print(train_x.shape)\n# print(train_y_categorical.shape)\n\n# # fit the model\n# history = model.fit(train_x, train_y_categorical, epochs=100, batch_size=64, validation_split=0.2)\n\n# # Evaluate the model on the test set\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T19:59:28.522651Z","iopub.status.idle":"2024-01-20T19:59:28.523063Z","shell.execute_reply.started":"2024-01-20T19:59:28.522866Z","shell.execute_reply":"2024-01-20T19:59:28.522885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# plt.plot(history.history['accuracy'])\n# plt.ylim(0,0.6)\n\n\n# plt.title('Model accuracy')\n# plt.ylabel('Accuracy')\n# plt.xlabel('Epoch')\n# plt.legend(['Train'], loc='upper left')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-20T19:59:28.524485Z","iopub.status.idle":"2024-01-20T19:59:28.525209Z","shell.execute_reply.started":"2024-01-20T19:59:28.525000Z","shell.execute_reply":"2024-01-20T19:59:28.525021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # y_pred = model.predict(X_test)\n# # #Converting predictions to label\n# # pred = list()\n# # for i in range(len(y_pred)):\n# #     pred.append(np.argmax(y_pred[i]))\n# # #Converting one hot encoded test label to label\n# # test = list()\n# # for i in range(len(y_test)):\n# #     test.append(np.argmax(y_test[i]))\n# #     from sklearn.metrics import accuracy_score\n# # a = accuracy_score(pred,test)\n# # print('Accuracy is:', a*100)\n# test_loss, test_accuracy = model.evaluate(test_x, test_y_categorical)\n# print(f'Test Accuracy: {test_accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-01-20T19:59:28.526492Z","iopub.status.idle":"2024-01-20T19:59:28.526869Z","shell.execute_reply.started":"2024-01-20T19:59:28.526676Z","shell.execute_reply":"2024-01-20T19:59:28.526693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.ylim(0,1)\n# plt.title('Model loss')\n# plt.ylabel('Loss')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Test'], loc='upper left')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-20T19:59:28.528302Z","iopub.status.idle":"2024-01-20T19:59:28.528690Z","shell.execute_reply.started":"2024-01-20T19:59:28.528503Z","shell.execute_reply":"2024-01-20T19:59:28.528520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# new_data = np.array([[1.0, 2.0], [0.5, 1.5]])\n\n# # Predict probabilities for each class\n# predictions = model.predict(new_data)\n\n# # Convert predicted probabilities to class labels\n# predicted_labels = np.argmax(predictions, axis=1)\n\n# # Display the results\n# print(\"New Data:\")\n# print(new_data)\n# print(\"Predicted Probabilities:\")\n# print(predictions)\n# print(\"Predicted Labels:\")\n# print(predicted_labels/5)","metadata":{"execution":{"iopub.status.busy":"2024-01-20T19:59:28.529895Z","iopub.status.idle":"2024-01-20T19:59:28.530256Z","shell.execute_reply.started":"2024-01-20T19:59:28.530075Z","shell.execute_reply":"2024-01-20T19:59:28.530091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}